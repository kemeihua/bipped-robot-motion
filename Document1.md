


https://www.cnblogs.com/emergence/p/19043338

强化学习/模仿学习/VLA/世界模型/扩散模型


https://zhuanlan.zhihu.com/p/1894416052961656917
模仿学习


Retargeting
    输入人类RGB视频；
    用计算机视觉工具提取简单人类动作；
    用RO工具过滤动作，确保真实性；
    重定向：将过滤后的人类动作转化为G1人形机器人的动作；
    模拟到真实迁移：将动作部署到真实机器人。
    
    
 https://zhuanlan.zhihu.com/p/1975147503319008690
  人类数据和物理接地
  通过一层“物理接地”处理，将人类数据转化为物理上可行的形式，就能将学到的策略部署到真实机器人上——比如机器人手完成相同任务、带机械臂的无人机（空中操作器）完成任务、人形机器人完成跑酷或局部操作。
  从模型基、模型感知到无模型
  
  模仿和强化学习结合（IRL和MIL）
  全身控制/舞蹈/步态
  
 
  
  接触动力学
 模仿学习输入输出：
 
 
 https://zhuanlan.zhihu.com/p/1885594346335737231
 基于运动学约束，从数据集中获取人类动作数据，将其 “重定向” 到适合机器人执行的关节级动作，转换为机器人可执行的运动命令，这些命令包含每个关节的目标位置或姿态变化
 
 总结
所以让机器人学习到跳舞的背后主要步骤包括：

数据集构建：通过一套较为昂贵的动作捕捉系统把人编排的舞蹈转化为骨骼各关节点的轨迹图，当然最简单的方法是直接找开源的数据。
运动命令生成与转换：从数据集中获取人类动作数据，将其 “重定向” 到适合机器人执行的关节级动作，转换为机器人可执行的运动命令，这些命令包含每个关节的目标位置或姿态变化，再输入到基于 Transformer 的编码器中，与机器人的历史感知信息（如当前的关节角度、速度、加速度等）一起生成用于训练的动作序列。
动作学习：通过模仿学习或强化学习的方法，训练一个用传统PID+EKF难以实现的神经网络，实现一套与预设动作一样的策略控制器。
仿真到现实迁移：通过领域随机化技术增强训练的鲁棒性，将在仿真环境中学会的计算和输出每个关节扭矩的策略迁移到实际的机器人上进行测试和验证，使机器人能够在真实环境中执行训练时学到的动作。


重定向数据：直接动捕数据（开源数据效果较差）
模仿学习：数据处理，设置奖励函数，输出动作（逆强化学习）

机器人舞蹈实现流程：
主要步骤

    动作捕捉数据获取：
        使用动作捕捉系统（如 Vicon、OptiTrack 或其他设备）来录制舞者的动作。
        导出录制数据为常用格式（例如 BVH、FBX、CSV 等）。

    数据预处理：
        解析捕捉到的数据，去除噪声，进行标准化处理，使数据适应到机器人控制的规格。

    动作映射和重定向：
        将捕捉的数据映射到机器人的控制空间，可能需要进行插值处理以适应机器人的关节限制和运动学特性。
        根据机器人模型进行适当的重定向，保持动作的流畅性和自然性。
        
    模型设计
    
     训练
     
    评估与优化

    机器人控制：
        使用适当的控制算法（如 PID 控制、学习控制或基于观察者的控制策略）来实现机器人的运动。
    m_torques: motor torques(电机扭矩)
    q_torques: 关节扭矩（m2j 电机扭矩转关节扭矩 雅可比矩阵）
   强化学习/模仿学习实现动作输出，通过控制器实现对应扭矩控制。
      舞蹈使用跟踪控制器
      
    实时测试与优化：
        在仿真环境中对机器人动作进行测试，根据表现进行必要的调整和优化。


1.2 奖励函数推断

    将专家的行为建模为一个策略 π∗(a∣s)π∗(a∣s)，表示在状态 ss 下选择动作 aa 的概率。
    IRL的目标是推断奖励函数 R(s)R(s)，使得专家的策略最大化累积奖励。
    可以利用相应的强化学习算法生成的价值函数 V(s)V(s) 来反推奖励。
    
模仿学习和强化学习结合（逆强化学习）

模仿学习中的课程学习是一种有效的策略，通过分阶段的训练方式，可以提高学习效率和最终的模型性能。通过将复杂任务分解为更小的部分，使得模仿学习过程变得更为高效和可控，从而帮助模型在实际操作环境中展现更好表现。

数据：
1. 高质量真机采集数据
2.仿真数据
3.互联网数据

模仿学习+强化学习+真机数据集实现VLA



